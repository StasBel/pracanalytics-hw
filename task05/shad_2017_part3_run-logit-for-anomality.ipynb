{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline  \n",
    "import numpy.ma as ma\n",
    "\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC, GradientBoostingClassifier as GBC\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data  \n",
    "data3_rows = np.loadtxt('/home/dante/SHAD/dataset3',delimiter='\\t')\n",
    "data3_cols = data3_rows.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define abnormality as rate of \"how often does current object appear in the tails of distributions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We have 191 abnormal users\n"
     ]
    }
   ],
   "source": [
    "# calculate abnormality\n",
    "p1=[]\n",
    "p5=[]\n",
    "p95=[]\n",
    "p99=[]\n",
    "\n",
    "N = data3_rows.shape[0]\n",
    "K = data3_rows.shape[1]\n",
    "\n",
    "for i in range(0,K):\n",
    "    p1.append(np.percentile(data3_cols[i], 1))\n",
    "    p5.append(np.percentile(data3_cols[i], 5))\n",
    "    p95.append(np.percentile(data3_cols[i], 95))\n",
    "    p99.append(np.percentile(data3_cols[i], 99))\n",
    "\n",
    "#print p1, p5,p95,p99\n",
    "\n",
    "abnormality = []\n",
    "for i in range(0, N):\n",
    "    abn = 0\n",
    "    for j in range(0, K):\n",
    "        c = data3_rows[i][j]\n",
    "        # 1 point of abnormalito for getting into top or bottom 5%\n",
    "        # +2 extra points of abnormality for getting into top or bottom 1%\n",
    "        abn += 2*(c < p1[j]) + (c < p5[j]) + (c > p95[j]) + 2*(c > p99[j]) \n",
    "    \n",
    "    # let's make binary abnormality, mark everybody with >=6 abn points\n",
    "    abnormality.append( (abn >=6)+0 )\n",
    "\n",
    "print '\\nWe have',np.sum(abnormality),'abnormal users'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to fit abnormality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "Logit = lm.LogisticRegression(\n",
    "    C=1.0, \n",
    "    class_weight=None, \n",
    "    dual=False, \n",
    "    fit_intercept=True, \n",
    "    intercept_scaling=1, \n",
    "    penalty='l2', \n",
    "    random_state=1, \n",
    "    tol=0.001\n",
    ")\n",
    "RF = RFC()\n",
    "GBC = GBC()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.logistic.LogisticRegression'> score: test\t0.987090367428\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'> score: learn\t0.983\n",
      "\n",
      "Distribution of labels\n",
      "(array([3816,  191]), array([ 0. ,  0.5,  1. ]))\n",
      "\n",
      "Distribution of predicted labels\n",
      "(array([3836,  171]), array([ 0. ,  0.5,  1. ]))\n"
     ]
    }
   ],
   "source": [
    "features = data3_rows\n",
    "labels = abnormality\n",
    "\n",
    "N_learn = 3000\n",
    "learn_features = features[:N_learn]\n",
    "test_features =  features[N_learn:]\n",
    "learn_labels = labels[:N_learn]\n",
    "test_labels =  labels[N_learn:]\n",
    "\n",
    "cur_method = Logit\n",
    "cur_method.fit(learn_features, learn_labels)\n",
    "\n",
    "predict = cur_method.predict(features)\n",
    "\n",
    "# print 'logit score: test\\t%s' % Logit.score(test_features, test_labels)\n",
    "# print 'logit score: learn\\t%s' % Logit.score(learn_features, learn_labels)\n",
    "print '%s score: test\\t%s' % (cur_method.__class__, cur_method.score(test_features, test_labels))\n",
    "print '%s score: learn\\t%s' % (cur_method.__class__, cur_method.score(learn_features, learn_labels))\n",
    "\n",
    "\n",
    "print \"\\nDistribution of labels\"\n",
    "print np.histogram(labels,2)\n",
    "print \"\\nDistribution of predicted labels\"\n",
    "print np.histogram(predict,2)\n",
    "\n",
    "# examples of \n",
    "# print predict[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate precision, recall, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
      "[[ 1.          0.81623447]\n",
      " [ 0.81623447  1.        ]]\n",
      "\n",
      "Abnormalities 191\n",
      "Predicted abnormalities 171\n",
      "Correct predictions for abnormalities 149\n",
      "\n",
      "Precision of abnormality detection  0.87134502924\n",
      "Recall of abnormality detection  0.780104712042\n",
      "\n",
      "F1-measure 0.82320441989\n",
      "\n",
      "Overall stats:\n",
      "Right verdicts 3943\n",
      "Wrong verdicts 64\n"
     ]
    }
   ],
   "source": [
    "# estimate whether our fit is good or not\n",
    "print list(predict)[:20]\n",
    "print labels[:20]\n",
    "print np.corrcoef(predict, labels)\n",
    "\n",
    "print '\\nAbnormalities', np.sum(labels)\n",
    "print 'Predicted abnormalities', np.sum(predict)\n",
    "print 'Correct predictions for abnormalities', np.sum(predict*labels)\n",
    "\n",
    "prec = np.sum(predict*labels)/(np.sum(predict)+.0)\n",
    "recall = np.sum(predict*labels)/(np.sum(labels)+.0)\n",
    "print '\\nPrecision of abnormality detection ', prec\n",
    "print 'Recall of abnormality detection ', recall\n",
    "print \"\\nF1-measure\", 2*prec*recall/(prec+recall)\n",
    "\n",
    "\n",
    "print '\\nOverall stats:'\n",
    "print 'Right verdicts', np.sum(predict==labels)\n",
    "print 'Wrong verdicts', np.sum(predict!=labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
